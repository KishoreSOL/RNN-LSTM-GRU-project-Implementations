{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '#': 3, \"'\": 4, '(': 5, ')': 6, '*': 7, ',': 8, '-': 9, '.': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '7': 16, '8': 17, ':': 18, ';': 19, '?': 20, '[': 21, ']': 22, '_': 23, 'a': 24, 'b': 25, 'c': 26, 'd': 27, 'e': 28, 'f': 29, 'g': 30, 'h': 31, 'i': 32, 'j': 33, 'k': 34, 'l': 35, 'm': 36, 'n': 37, 'o': 38, 'p': 39, 'q': 40, 'r': 41, 's': 42, 't': 43, 'u': 44, 'v': 45, 'w': 46, 'x': 47, 'y': 48, 'z': 49, 'ù': 50, '—': 51, '‘': 52, '’': 53, '“': 54, '”': 55}\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "print(char_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  117572\n",
      "Total Vocab:  56\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print( \"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  117552\n"
     ]
    }
   ],
   "source": [
    "seq_length = 20\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    " seq_in = raw_text[i:i + seq_length]\n",
    " seq_out = raw_text[i + seq_length]\n",
    " dataX.append([char_to_int[char] for char in seq_in])\n",
    " dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Deep learning\\Recurrent Neural Networks Project\\env\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint\n",
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.keras\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 3.1242\n",
      "Epoch 1: loss improved from inf to 3.04943, saving model to weights-improvement-01-3.0494.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 56ms/step - loss: 3.1241\n",
      "Epoch 2/20\n",
      "\u001b[1m918/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 2.8881\n",
      "Epoch 2: loss improved from 3.04943 to 2.86548, saving model to weights-improvement-02-2.8655.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 52ms/step - loss: 2.8881\n",
      "Epoch 3/20\n",
      "\u001b[1m918/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 2.7983\n",
      "Epoch 3: loss improved from 2.86548 to 2.78273, saving model to weights-improvement-03-2.7827.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 58ms/step - loss: 2.7983\n",
      "Epoch 4/20\n",
      "\u001b[1m918/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 2.7264\n",
      "Epoch 4: loss improved from 2.78273 to 2.71634, saving model to weights-improvement-04-2.7163.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 59ms/step - loss: 2.7264\n",
      "Epoch 5/20\n",
      "\u001b[1m918/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 2.6704\n",
      "Epoch 5: loss improved from 2.71634 to 2.66252, saving model to weights-improvement-05-2.6625.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 58ms/step - loss: 2.6704\n",
      "Epoch 6/20\n",
      "\u001b[1m918/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 2.6238\n",
      "Epoch 6: loss improved from 2.66252 to 2.61947, saving model to weights-improvement-06-2.6195.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 58ms/step - loss: 2.6238\n",
      "Epoch 7/20\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 2.5805\n",
      "Epoch 7: loss improved from 2.61947 to 2.57931, saving model to weights-improvement-07-2.5793.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 54ms/step - loss: 2.5805\n",
      "Epoch 8/20\n",
      "\u001b[1m918/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 2.5415\n",
      "Epoch 8: loss improved from 2.57931 to 2.53806, saving model to weights-improvement-08-2.5381.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 55ms/step - loss: 2.5415\n",
      "Epoch 9/20\n",
      "\u001b[1m918/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 2.4999\n",
      "Epoch 9: loss improved from 2.53806 to 2.49972, saving model to weights-improvement-09-2.4997.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 52ms/step - loss: 2.4999\n",
      "Epoch 10/20\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 2.4671\n",
      "Epoch 10: loss improved from 2.49972 to 2.46294, saving model to weights-improvement-10-2.4629.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 53ms/step - loss: 2.4671\n",
      "Epoch 11/20\n",
      "\u001b[1m918/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 2.4228\n",
      "Epoch 11: loss improved from 2.46294 to 2.42263, saving model to weights-improvement-11-2.4226.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 53ms/step - loss: 2.4228\n",
      "Epoch 12/20\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 2.3852\n",
      "Epoch 12: loss improved from 2.42263 to 2.38549, saving model to weights-improvement-12-2.3855.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 53ms/step - loss: 2.3852\n",
      "Epoch 13/20\n",
      "\u001b[1m918/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 2.3444\n",
      "Epoch 13: loss improved from 2.38549 to 2.34911, saving model to weights-improvement-13-2.3491.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 53ms/step - loss: 2.3444\n",
      "Epoch 14/20\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 2.3076\n",
      "Epoch 14: loss improved from 2.34911 to 2.31410, saving model to weights-improvement-14-2.3141.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 53ms/step - loss: 2.3076\n",
      "Epoch 15/20\n",
      "\u001b[1m918/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 2.2788\n",
      "Epoch 15: loss improved from 2.31410 to 2.28034, saving model to weights-improvement-15-2.2803.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 53ms/step - loss: 2.2788\n",
      "Epoch 16/20\n",
      "\u001b[1m918/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 2.2359\n",
      "Epoch 16: loss improved from 2.28034 to 2.24615, saving model to weights-improvement-16-2.2462.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 53ms/step - loss: 2.2360\n",
      "Epoch 17/20\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 2.1968\n",
      "Epoch 17: loss improved from 2.24615 to 2.21163, saving model to weights-improvement-17-2.2116.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 54ms/step - loss: 2.1969\n",
      "Epoch 18/20\n",
      "\u001b[1m918/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 2.1712\n",
      "Epoch 18: loss improved from 2.21163 to 2.17729, saving model to weights-improvement-18-2.1773.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 54ms/step - loss: 2.1713\n",
      "Epoch 19/20\n",
      "\u001b[1m918/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 2.1348\n",
      "Epoch 19: loss improved from 2.17729 to 2.14616, saving model to weights-improvement-19-2.1462.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 57ms/step - loss: 2.1349\n",
      "Epoch 20/20\n",
      "\u001b[1m918/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 2.1019\n",
      "Epoch 20: loss improved from 2.14616 to 2.11453, saving model to weights-improvement-20-2.1145.keras\n",
      "\u001b[1m919/919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 58ms/step - loss: 2.1020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26eb2594450>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Deep learning\\Recurrent Neural Networks Project\\env\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-20-2.1145.keras\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" the sky.\n",
      "\n",
      "alice went \"\n",
      " he aalitd d toin wfll oase \n",
      "ant the was aolntg to the ctoreusin of the crold an tou cank an toe tail tf the keoe  the wosed tai iowt the cate pat ie a lotte, and the dadt was ooteigg an the coold  she fad not deei to the thete was to ali toene th theeg aeain, and wast dnlning ano the rase thieg war soeee oo the taaei  and toened an the coold  she fad not deei to the thete was to ali toene th theeg aeain, and wast dnlning ano the rase thieg war soeee oo the taaei  and toened an the coold  she fad not deei to the thete was to ali toene th theeg aeain, and wast dnlning ano the rase thieg war soeee oo the taaei  and toened an the coold  she fad not deei to the thete was to ali toene th theeg aeain, and wast dnlning ano the rase thieg war soeee oo the taaei  and toened an the coold  she fad not deei to the thete was to ali toene th theeg aeain, and wast dnlning ano the rase thieg war soeee oo the taaei  and toened an the coold  she fad not deei to the thete was to ali toene th theeg aeain,\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
